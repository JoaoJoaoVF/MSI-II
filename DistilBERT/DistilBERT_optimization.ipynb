{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detec√ß√£o de Ataques de Rede em Tempo Real com DistilBERT Otimizado\n",
    "\n",
    "Este notebook demonstra como adaptar o DistilBERT para detec√ß√£o de ataques DDoS em tempo real usando dados de rede tabulares.\n",
    "\n",
    "**Objetivo**: Criar um modelo leve e eficiente para rodar em Raspberry Pi detectando ataques em tempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Instala√ß√£o de Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar se estamos no Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import files\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"Executando no Google Colab: {IN_COLAB}\")\n",
    "\n",
    "# Instalar depend√™ncias\n",
    "!pip install -q transformers torch onnx onnxruntime numpy pandas scikit-learn matplotlib seaborn optimum[onnxruntime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importar bibliotecas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertConfig, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload e An√°lise dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Fa√ßa upload dos seus arquivos CSV de dados de rede:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Listar arquivos carregados\n",
    "    csv_files = [f for f in uploaded.keys() if f.endswith('.csv')]\n",
    "    print(f\"Arquivos CSV carregados: {csv_files}\")\n",
    "else:\n",
    "    # Para execu√ß√£o local, listar arquivos CSV no diret√≥rio\n",
    "    import glob\n",
    "    csv_files = glob.glob(\"*.csv\")\n",
    "    print(f\"Arquivos CSV encontrados: {csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_analyze_data(csv_files, sample_size=50000):\n",
    "    \"\"\"Carregar e analisar dados de m√∫ltiplos arquivos CSV\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for file in csv_files[:5]:  # Limitar a 5 arquivos para teste\n",
    "        print(f\"Carregando {file}...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Amostrar dados para reduzir tempo de processamento\n",
    "            if len(df) > sample_size:\n",
    "                df = df.sample(n=sample_size, random_state=42)\n",
    "            all_data.append(df)\n",
    "            print(f\"  - Shape: {df.shape}\")\n",
    "            print(f\"  - Labels √∫nicos: {df['label'].unique()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Erro ao carregar {file}: {e}\")\n",
    "    \n",
    "    # Combinar todos os dados\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\nDados combinados: {combined_df.shape}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"Nenhum dado foi carregado com sucesso.\")\n",
    "        return None\n",
    "\n",
    "# Carregar dados\n",
    "df = load_and_analyze_data(csv_files)\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\n=== AN√ÅLISE DOS DADOS ===\")\n",
    "    print(f\"Shape total: {df.shape}\")\n",
    "    print(f\"\\nColunas: {list(df.columns)}\")\n",
    "    print(f\"\\nDistribui√ß√£o de labels:\")\n",
    "    print(df['label'].value_counts())\n",
    "    print(f\"\\nValores nulos por coluna:\")\n",
    "    print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pr√©-processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Pr√©-processar dados para o modelo\"\"\"\n",
    "    \n",
    "    # Remover colunas com muitos valores nulos ou constantes\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remover colunas com mais de 50% de valores nulos\n",
    "    null_threshold = 0.5\n",
    "    null_cols = df_clean.columns[df_clean.isnull().mean() > null_threshold]\n",
    "    df_clean = df_clean.drop(columns=null_cols)\n",
    "    print(f\"Removidas {len(null_cols)} colunas com muitos valores nulos\")\n",
    "    \n",
    "    # Preencher valores nulos restantes\n",
    "    df_clean = df_clean.fillna(0)\n",
    "    \n",
    "    # Separar features e labels\n",
    "    X = df_clean.drop('label', axis=1)\n",
    "    y = df_clean['label']\n",
    "    \n",
    "    # Codificar labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Normalizar features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(f\"\\nFeatures shape: {X_scaled.shape}\")\n",
    "    print(f\"Labels shape: {y_encoded.shape}\")\n",
    "    print(f\"N√∫mero de classes: {len(label_encoder.classes_)}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    return X_scaled, y_encoded, label_encoder, scaler, list(X.columns)\n",
    "\n",
    "# Pr√©-processar dados\n",
    "if df is not None:\n",
    "    X, y, label_encoder, scaler, feature_names = preprocess_data(df)\n",
    "    \n",
    "    # Dividir em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTreino: {X_train.shape}, Teste: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo DistilBERT Adaptado para Dados Tabulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class TabularDistilBERT(nn.Module):\n",
    "    \"\"\"DistilBERT adaptado para dados tabulares de rede\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, num_classes, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Configura√ß√£o reduzida do DistilBERT para efici√™ncia\n",
    "        config = DistilBertConfig(\n",
    "            vocab_size=1000,  # Reduzido\n",
    "            max_position_embeddings=512,\n",
    "            n_layers=3,  # Reduzido de 6 para 3\n",
    "            n_heads=8,\n",
    "            dim=hidden_dim,  # Reduzido\n",
    "            hidden_dim=hidden_dim * 4,\n",
    "            dropout=0.1,\n",
    "            attention_dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Camada de proje√ß√£o para converter features tabulares em embeddings\n",
    "        self.feature_projection = nn.Linear(num_features, hidden_dim)\n",
    "        \n",
    "        # DistilBERT backbone (sem embeddings de palavras)\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        \n",
    "        # Cabe√ßa de classifica√ß√£o\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Projetar features para dimens√£o do modelo\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Converter features em embeddings\n",
    "        embeddings = self.feature_projection(features)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Adicionar dimens√£o de sequ√™ncia (simular tokens)\n",
    "        embeddings = embeddings.unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "        \n",
    "        # Criar attention mask\n",
    "        attention_mask = torch.ones(batch_size, 1, device=features.device)\n",
    "        \n",
    "        # Passar pelo DistilBERT\n",
    "        outputs = self.distilbert(\n",
    "            inputs_embeds=embeddings,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Usar o √∫ltimo hidden state\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Classifica√ß√£o\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Criar modelo\n",
    "if df is not None:\n",
    "    num_features = X.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    model = TabularDistilBERT(num_features, num_classes, hidden_dim=128)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Modelo criado:\")\n",
    "    print(f\"  - Features de entrada: {num_features}\")\n",
    "    print(f\"  - Classes de sa√≠da: {num_classes}\")\n",
    "    print(f\"  - Par√¢metros totais: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset e DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class NetworkDataset(Dataset):\n",
    "    \"\"\"Dataset para dados de rede\"\"\"\n",
    "    \n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': self.features[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Criar datasets\n",
    "if df is not None:\n",
    "    train_dataset = NetworkDataset(X_train, y_train)\n",
    "    test_dataset = NetworkDataset(X_test, y_test)\n",
    "    \n",
    "    # Criar dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(f\"Dataset de treino: {len(train_dataset)} amostras\")\n",
    "    print(f\"Dataset de teste: {len(test_dataset)} amostras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs=5):\n",
    "    \"\"\"Treinar o modelo\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f\"√âpoca {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "        \n",
    "        # Avaliar no conjunto de teste\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            test_acc = evaluate_model(model, test_loader)\n",
    "            print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Avaliar o modelo\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    model.train()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# Treinar modelo\n",
    "if df is not None:\n",
    "    print(\"Iniciando treinamento...\")\n",
    "    train_losses, train_accuracies = train_model(model, train_loader, test_loader, num_epochs=3)\n",
    "    \n",
    "    # Avalia√ß√£o final\n",
    "    final_accuracy = evaluate_model(model, test_loader)\n",
    "    print(f\"\\nAccuracy final no teste: {final_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An√°lise Detalhada de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detailed_evaluation(model, test_loader, label_encoder):\n",
    "    \"\"\"Avalia√ß√£o detalhada com m√©tricas\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Medir tempo de infer√™ncia\n",
    "            start_time = time.time()\n",
    "            outputs = model(features)\n",
    "            inference_time = (time.time() - start_time) * 1000  # ms\n",
    "            inference_times.append(inference_time)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    print(\"\\n=== RELAT√ìRIO DE PERFORMANCE ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Tempo m√©dio de infer√™ncia: {np.mean(inference_times):.2f} ms\")\n",
    "    print(f\"Throughput: {1000/np.mean(inference_times):.2f} predi√ß√µes/segundo\")\n",
    "    \n",
    "    # Relat√≥rio de classifica√ß√£o\n",
    "    print(\"\\n=== RELAT√ìRIO DE CLASSIFICA√á√ÉO ===\")\n",
    "    print(classification_report(\n",
    "        all_labels, all_predictions, \n",
    "        target_names=label_encoder.classes_,\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    # Matriz de confus√£o\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.title('Matriz de Confus√£o')\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, np.mean(inference_times)\n",
    "\n",
    "# Avalia√ß√£o detalhada\n",
    "if df is not None:\n",
    "    accuracy, avg_inference_time = detailed_evaluation(model, test_loader, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Otimiza√ß√£o para Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class OptimizedNetworkDetector(nn.Module):\n",
    "    \"\"\"Wrapper otimizado para exporta√ß√£o ONNX\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, features):\n",
    "        logits = self.model(features)\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        return logits, probabilities\n",
    "\n",
    "def export_optimized_model(model, X_sample, scaler, label_encoder, feature_names):\n",
    "    \"\"\"Exportar modelo otimizado para ONNX\"\"\"\n",
    "    \n",
    "    # Criar wrapper otimizado\n",
    "    optimized_model = OptimizedNetworkDetector(model)\n",
    "    optimized_model.eval()\n",
    "    \n",
    "    # Preparar input de exemplo\n",
    "    dummy_input = torch.FloatTensor(X_sample[:1]).to(device)\n",
    "    \n",
    "    # Exportar para ONNX\n",
    "    torch.onnx.export(\n",
    "        optimized_model,\n",
    "        dummy_input,\n",
    "        'network_attack_detector.onnx',\n",
    "        input_names=['features'],\n",
    "        output_names=['logits', 'probabilities'],\n",
    "        dynamic_axes={\n",
    "            'features': {0: 'batch_size'},\n",
    "            'logits': {0: 'batch_size'},\n",
    "            'probabilities': {0: 'batch_size'}\n",
    "        },\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo exportado para ONNX: network_attack_detector.onnx\")\n",
    "    \n",
    "    # Quantizar modelo\n",
    "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "    \n",
    "    quantize_dynamic(\n",
    "        'network_attack_detector.onnx',\n",
    "        'network_attack_detector_quantized.onnx',\n",
    "        weight_type=QuantType.QInt8\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo quantizado: network_attack_detector_quantized.onnx\")\n",
    "    \n",
    "    # Salvar metadados\n",
    "    import pickle\n",
    "    \n",
    "    metadata = {\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': label_encoder,\n",
    "        'feature_names': feature_names,\n",
    "        'num_features': len(feature_names),\n",
    "        'num_classes': len(label_encoder.classes_),\n",
    "        'classes': label_encoder.classes_.tolist()\n",
    "    }\n",
    "    \n",
    "    with open('model_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    print(\"Metadados salvos: model_metadata.pkl\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Exportar modelo otimizado\n",
    "if df is not None:\n",
    "    print(\"Exportando modelo otimizado...\")\n",
    "    metadata = export_optimized_model(model, X_test, scaler, label_encoder, feature_names)\n",
    "    \n",
    "    # Verificar tamanhos dos arquivos\n",
    "    import os\n",
    "    original_size = os.path.getsize('network_attack_detector.onnx') / (1024*1024)\n",
    "    quantized_size = os.path.getsize('network_attack_detector_quantized.onnx') / (1024*1024)\n",
    "    \n",
    "    print(f\"\\nTamanho do modelo original: {original_size:.2f} MB\")\n",
    "    print(f\"Tamanho do modelo quantizado: {quantized_size:.2f} MB\")\n",
    "    print(f\"Redu√ß√£o: {(1 - quantized_size/original_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sistema de Monitoramento em Tempo Real para Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar script de monitoramento em tempo real\n",
    "monitoring_script = '''\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Sistema de Detec√ß√£o de Ataques de Rede em Tempo Real\n",
    "Otimizado para Raspberry Pi\n",
    "\"\"\"\n",
    "\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import queue\n",
    "import sys\n",
    "\n",
    "class NetworkAttackDetector:\n",
    "    def __init__(self, model_path, metadata_path):\n",
    "        \"\"\"Inicializar detector de ataques\"\"\"\n",
    "        \n",
    "        print(\"Carregando modelo...\")\n",
    "        self.session = ort.InferenceSession(model_path)\n",
    "        \n",
    "        print(\"Carregando metadados...\")\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "        \n",
    "        self.scaler = self.metadata['scaler']\n",
    "        self.label_encoder = self.metadata['label_encoder']\n",
    "        self.feature_names = self.metadata['feature_names']\n",
    "        self.classes = self.metadata['classes']\n",
    "        \n",
    "        print(f\"Modelo carregado com sucesso!\")\n",
    "        print(f\"Classes detect√°veis: {self.classes}\")\n",
    "        \n",
    "        # Estat√≠sticas\n",
    "        self.total_predictions = 0\n",
    "        self.attack_detections = 0\n",
    "        self.inference_times = []\n",
    "    \n",
    "    def preprocess_features(self, features_dict):\n",
    "        \"\"\"Pr√©-processar features de entrada\"\"\"\n",
    "        \n",
    "        # Converter para array na ordem correta\n",
    "        features_array = np.array([features_dict.get(name, 0.0) for name in self.feature_names])\n",
    "        \n",
    "        # Normalizar\n",
    "        features_scaled = self.scaler.transform(features_array.reshape(1, -1))\n",
    "        \n",
    "        return features_scaled.astype(np.float32)\n",
    "    \n",
    "    def predict(self, features_dict):\n",
    "        \"\"\"Fazer predi√ß√£o de ataque\"\"\"\n",
    "        \n",
    "        # Pr√©-processar\n",
    "        features = self.preprocess_features(features_dict)\n",
    "        \n",
    "        # Infer√™ncia\n",
    "        start_time = time.time()\n",
    "        ort_inputs = {'features': features}\n",
    "        logits, probabilities = self.session.run(None, ort_inputs)\n",
    "        inference_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Processar resultado\n",
    "        predicted_class_idx = np.argmax(probabilities[0])\n",
    "        predicted_class = self.classes[predicted_class_idx]\n",
    "        confidence = probabilities[0][predicted_class_idx]\n",
    "        \n",
    "        # Atualizar estat√≠sticas\n",
    "        self.total_predictions += 1\n",
    "        self.inference_times.append(inference_time)\n",
    "        \n",
    "        if predicted_class != 'Benign':  # Assumindo que 'Benign' √© tr√°fego normal\n",
    "            self.attack_detections += 1\n",
    "        \n",
    "        return {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'predicted_class': predicted_class,\n",
    "            'confidence': float(confidence),\n",
    "            'is_attack': predicted_class != 'Benign',\n",
    "            'inference_time_ms': inference_time,\n",
    "            'all_probabilities': probabilities[0].tolist()\n",
    "        }\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Obter estat√≠sticas do detector\"\"\"\n",
    "        \n",
    "        if not self.inference_times:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'total_predictions': self.total_predictions,\n",
    "            'attack_detections': self.attack_detections,\n",
    "            'attack_rate': self.attack_detections / self.total_predictions if self.total_predictions > 0 else 0,\n",
    "            'avg_inference_time_ms': np.mean(self.inference_times),\n",
    "            'max_inference_time_ms': np.max(self.inference_times),\n",
    "            'min_inference_time_ms': np.min(self.inference_times),\n",
    "            'throughput_per_second': 1000 / np.mean(self.inference_times)\n",
    "        }\n",
    "\n",
    "class RealTimeMonitor:\n",
    "    def __init__(self, detector, log_file='attack_log.json'):\n",
    "        self.detector = detector\n",
    "        self.log_file = log_file\n",
    "        self.data_queue = queue.Queue()\n",
    "        self.running = False\n",
    "    \n",
    "    def log_detection(self, result):\n",
    "        \"\"\"Registrar detec√ß√£o em arquivo\"\"\"\n",
    "        \n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "    \n",
    "    def process_data_stream(self):\n",
    "        \"\"\"Processar stream de dados em tempo real\"\"\"\n",
    "        \n",
    "        while self.running:\n",
    "            try:\n",
    "                # Obter dados da fila\n",
    "                features_dict = self.data_queue.get(timeout=1)\n",
    "                \n",
    "                # Fazer predi√ß√£o\n",
    "                result = self.detector.predict(features_dict)\n",
    "                \n",
    "                # Log se for ataque\n",
    "                if result['is_attack']:\n",
    "                    print(f\"üö® ATAQUE DETECTADO: {result['predicted_class']} (Confian√ßa: {result['confidence']:.3f})\")\n",
    "                    self.log_detection(result)\n",
    "                else:\n",
    "                    print(f\"‚úÖ Tr√°fego normal (Confian√ßa: {result['confidence']:.3f})\")\n",
    "                \n",
    "                self.data_queue.task_done()\n",
    "                \n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Erro no processamento: {e}\")\n",
    "    \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Iniciar monitoramento\"\"\"\n",
    "        \n",
    "        self.running = True\n",
    "        monitor_thread = threading.Thread(target=self.process_data_stream)\n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "        \n",
    "        print(\"Monitoramento iniciado...\")\n",
    "        return monitor_thread\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Parar monitoramento\"\"\"\n",
    "        self.running = False\n",
    "    \n",
    "    def add_data(self, features_dict):\n",
    "        \"\"\"Adicionar dados para an√°lise\"\"\"\n",
    "        self.data_queue.put(features_dict)\n",
    "\n",
    "def simulate_network_data(csv_file, detector, monitor, delay=1.0):\n",
    "    \"\"\"Simular dados de rede em tempo real\"\"\"\n",
    "    \n",
    "    print(f\"Carregando dados de simula√ß√£o: {csv_file}\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(f\"Iniciando simula√ß√£o com {len(df)} amostras...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Converter linha para dicion√°rio (excluindo label)\n",
    "        features_dict = row.drop('label').to_dict()\n",
    "        \n",
    "        # Adicionar √† fila de monitoramento\n",
    "        monitor.add_data(features_dict)\n",
    "        \n",
    "        # Mostrar progresso\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            stats = detector.get_statistics()\n",
    "            print(f\"\\nProcessadas {idx + 1} amostras\")\n",
    "            print(f\"Taxa de ataques: {stats.get('attack_rate', 0):.3f}\")\n",
    "            print(f\"Tempo m√©dio: {stats.get('avg_inference_time_ms', 0):.2f} ms\")\n",
    "        \n",
    "        time.sleep(delay)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Detector de Ataques de Rede em Tempo Real')\n",
    "    parser.add_argument('--model', default='network_attack_detector_quantized.onnx', help='Modelo ONNX')\n",
    "    parser.add_argument('--metadata', default='model_metadata.pkl', help='Metadados do modelo')\n",
    "    parser.add_argument('--simulate', type=str, help='Arquivo CSV para simula√ß√£o')\n",
    "    parser.add_argument('--delay', type=float, default=0.1, help='Delay entre amostras (segundos)')\n",
    "    parser.add_argument('--interactive', action='store_true', help='Modo interativo')\n",
    "    parser.add_argument('--benchmark', action='store_true', help='Benchmark de performance')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Inicializar detector\n",
    "    try:\n",
    "        detector = NetworkAttackDetector(args.model, args.metadata)\n",
    "        monitor = RealTimeMonitor(detector)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao inicializar detector: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    if args.benchmark:\n",
    "        # Benchmark de performance\n",
    "        print(\"Executando benchmark...\")\n",
    "        \n",
    "        # Criar dados de teste\n",
    "        test_features = {name: np.random.randn() for name in detector.feature_names}\n",
    "        \n",
    "        # Executar m√∫ltiplas predi√ß√µes\n",
    "        for i in range(1000):\n",
    "            detector.predict(test_features)\n",
    "        \n",
    "        stats = detector.get_statistics()\n",
    "        print(f\"\\nResultados do benchmark:\")\n",
    "        print(f\"Predi√ß√µes: {stats['total_predictions']}\")\n",
    "        print(f\"Tempo m√©dio: {stats['avg_inference_time_ms']:.2f} ms\")\n",
    "        print(f\"Throughput: {stats['throughput_per_second']:.2f} predi√ß√µes/segundo\")\n",
    "        \n",
    "    elif args.simulate:\n",
    "        # Simular dados de rede\n",
    "        monitor_thread = monitor.start_monitoring()\n",
    "        \n",
    "        try:\n",
    "            simulate_network_data(args.simulate, detector, monitor, args.delay)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nInterrompido pelo usu√°rio\")\n",
    "        finally:\n",
    "            monitor.stop_monitoring()\n",
    "            \n",
    "            # Mostrar estat√≠sticas finais\n",
    "            stats = detector.get_statistics()\n",
    "            print(f\"\\n=== ESTAT√çSTICAS FINAIS ===\")\n",
    "            for key, value in stats.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "    \n",
    "    elif args.interactive:\n",
    "        # Modo interativo\n",
    "        print(\"\\nModo interativo ativado.\")\n",
    "        print(\"Digite valores para as features ou 'sair' para encerrar.\")\n",
    "        print(f\"Features necess√°rias: {detector.feature_names[:5]}... (total: {len(detector.feature_names)})\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Exemplo simples: usar valores aleat√≥rios\n",
    "                input(\"Pressione Enter para gerar predi√ß√£o com dados aleat√≥rios (ou Ctrl+C para sair): \")\n",
    "                \n",
    "                test_features = {name: np.random.randn() for name in detector.feature_names}\n",
    "                result = detector.predict(test_features)\n",
    "                \n",
    "                print(f\"\\nResultado:\")\n",
    "                print(f\"  Classe: {result['predicted_class']}\")\n",
    "                print(f\"  Confian√ßa: {result['confidence']:.3f}\")\n",
    "                print(f\"  √â ataque: {result['is_attack']}\")\n",
    "                print(f\"  Tempo: {result['inference_time_ms']:.2f} ms\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "    \n",
    "    else:\n",
    "        print(\"Use --simulate, --interactive ou --benchmark\")\n",
    "        parser.print_help()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Salvar script\n",
    "with open('realtime_network_monitor.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(monitoring_script)\n",
    "\n",
    "print(\"Script de monitoramento criado: realtime_network_monitor.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Arquivos de Configura√ß√£o e Documenta√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar requirements.txt\n",
    "requirements = '''onnxruntime==1.15.1\n",
    "numpy==1.24.3\n",
    "pandas==2.0.3\n",
    "scikit-learn==1.3.0\n",
    "'''\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "# Criar script de instala√ß√£o\n",
    "install_script = '''#!/bin/bash\n",
    "# Script de instala√ß√£o para Raspberry Pi\n",
    "\n",
    "echo \"Instalando depend√™ncias para detec√ß√£o de ataques de rede...\"\n",
    "\n",
    "# Atualizar sistema\n",
    "sudo apt update\n",
    "sudo apt upgrade -y\n",
    "\n",
    "# Instalar Python e pip\n",
    "sudo apt install python3 python3-pip -y\n",
    "\n",
    "# Instalar depend√™ncias Python\n",
    "pip3 install -r requirements.txt\n",
    "\n",
    "# Criar diret√≥rio de logs\n",
    "mkdir -p logs\n",
    "\n",
    "echo \"Instala√ß√£o conclu√≠da!\"\n",
    "echo \"Para testar: python3 realtime_network_monitor.py --benchmark\"\n",
    "'''\n",
    "\n",
    "with open('install.sh', 'w') as f:\n",
    "    f.write(install_script)\n",
    "\n",
    "# Criar README detalhado\n",
    "readme = f'''# Sistema de Detec√ß√£o de Ataques de Rede em Tempo Real\n",
    "\n",
    "Sistema baseado em DistilBERT otimizado para detectar ataques DDoS em tempo real em dispositivos IoT como Raspberry Pi.\n",
    "\n",
    "## Caracter√≠sticas do Modelo\n",
    "\n",
    "- **Modelo**: DistilBERT adaptado para dados tabulares\n",
    "- **Otimiza√ß√µes**: Quantiza√ß√£o INT8, ONNX Runtime\n",
    "- **Tamanho**: ~{quantized_size:.1f} MB (redu√ß√£o de {(1 - quantized_size/original_size)*100:.1f}%)\n",
    "- **Performance**: ~{avg_inference_time:.1f} ms por predi√ß√£o\n",
    "- **Classes detect√°veis**: {len(label_encoder.classes_)} tipos de ataques\n",
    "\n",
    "## Tipos de Ataques Detectados\n",
    "\n",
    "{chr(10).join([f\"- {cls}\" for cls in label_encoder.classes_])}\n",
    "\n",
    "## Instala√ß√£o no Raspberry Pi\n",
    "\n",
    "1. Transfira todos os arquivos para o Raspberry Pi\n",
    "2. Execute o script de instala√ß√£o:\n",
    "```bash\n",
    "chmod +x install.sh\n",
    "./install.sh\n",
    "```\n",
    "\n",
    "## Uso\n",
    "\n",
    "### Benchmark de Performance\n",
    "```bash\n",
    "python3 realtime_network_monitor.py --benchmark\n",
    "```\n",
    "\n",
    "### Simula√ß√£o com Dados CSV\n",
    "```bash\n",
    "python3 realtime_network_monitor.py --simulate dados_rede.csv --delay 0.1\n",
    "```\n",
    "\n",
    "### Modo Interativo\n",
    "```bash\n",
    "python3 realtime_network_monitor.py --interactive\n",
    "```\n",
    "\n",
    "## Arquivos Inclu√≠dos\n",
    "\n",
    "- `network_attack_detector_quantized.onnx`: Modelo otimizado\n",
    "- `model_metadata.pkl`: Metadados (scaler, encoder, features)\n",
    "- `realtime_network_monitor.py`: Sistema de monitoramento\n",
    "- `requirements.txt`: Depend√™ncias Python\n",
    "- `install.sh`: Script de instala√ß√£o\n",
    "\n",
    "## M√©tricas de Performance\n",
    "\n",
    "- **Accuracy**: {accuracy:.3f}\n",
    "- **Tempo de Infer√™ncia**: {avg_inference_time:.2f} ms\n",
    "- **Throughput**: {1000/avg_inference_time:.1f} predi√ß√µes/segundo\n",
    "- **Uso de Mem√≥ria**: < 100 MB\n",
    "\n",
    "## Monitoramento em Tempo Real\n",
    "\n",
    "O sistema pode processar dados de rede em tempo real e:\n",
    "- Detectar ataques com alta precis√£o\n",
    "- Registrar detec√ß√µes em arquivo JSON\n",
    "- Mostrar estat√≠sticas em tempo real\n",
    "- Funcionar 24/7 em Raspberry Pi\n",
    "\n",
    "## Integra√ß√£o com Sistemas de Rede\n",
    "\n",
    "Para integrar com sistemas reais de monitoramento de rede:\n",
    "1. Capture pacotes com ferramentas como tcpdump/Wireshark\n",
    "2. Extraia features de rede (dura√ß√£o, flags, protocolos, etc.)\n",
    "3. Alimente o detector em tempo real\n",
    "4. Configure alertas para ataques detectados\n",
    "\n",
    "## Suporte\n",
    "\n",
    "Para d√∫vidas ou problemas, verifique:\n",
    "- Logs do sistema em `attack_log.json`\n",
    "- Compatibilidade das features de entrada\n",
    "- Vers√µes das depend√™ncias\n",
    "'''\n",
    "\n",
    "with open('README.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(\"Documenta√ß√£o criada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download dos Arquivos para Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if IN_COLAB and df is not None:\n",
    "    print(\"Preparando arquivos para download...\")\n",
    "    \n",
    "    # Lista de arquivos para download\n",
    "    files_to_download = [\n",
    "        'network_attack_detector_quantized.onnx',\n",
    "        'model_metadata.pkl',\n",
    "        'realtime_network_monitor.py',\n",
    "        'requirements.txt',\n",
    "        'install.sh',\n",
    "        'README.md'\n",
    "    ]\n",
    "    \n",
    "    # Download dos arquivos\n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            files.download(file)\n",
    "            print(f\"‚úÖ {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao baixar {file}: {e}\")\n",
    "    \n",
    "    print(\"\\n=== RESUMO DO SISTEMA ===\")\n",
    "    print(f\"üìä Accuracy do modelo: {accuracy:.3f}\")\n",
    "    print(f\"‚ö° Tempo de infer√™ncia: {avg_inference_time:.2f} ms\")\n",
    "    print(f\"üöÄ Throughput: {1000/avg_inference_time:.1f} predi√ß√µes/segundo\")\n",
    "    print(f\"üíæ Tamanho do modelo: {quantized_size:.1f} MB\")\n",
    "    print(f\"üéØ Classes detect√°veis: {len(label_encoder.classes_)}\")\n",
    "    \n",
    "    print(\"\\n=== PR√ìXIMOS PASSOS ===\")\n",
    "    print(\"1. Transfira todos os arquivos para o Raspberry Pi\")\n",
    "    print(\"2. Execute: chmod +x install.sh && ./install.sh\")\n",
    "    print(\"3. Teste: python3 realtime_network_monitor.py --benchmark\")\n",
    "    print(\"4. Use com seus dados: python3 realtime_network_monitor.py --simulate seu_arquivo.csv\")\n",
    "    \n",
    "elif df is not None:\n",
    "    print(\"\\nArquivos salvos localmente:\")\n",
    "    print(\"- network_attack_detector_quantized.onnx\")\n",
    "    print(\"- model_metadata.pkl\")\n",
    "    print(\"- realtime_network_monitor.py\")\n",
    "    print(\"- requirements.txt\")\n",
    "    print(\"- install.sh\")\n",
    "    print(\"- README.md\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado foi carregado. Fa√ßa upload dos arquivos CSV primeiro.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Detec√ß√£o de Ataques de Rede - DistilBERT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}