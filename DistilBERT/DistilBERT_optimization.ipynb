{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecção de Ataques de Rede em Tempo Real com DistilBERT Otimizado\n",
    "\n",
    "Este notebook demonstra como adaptar o DistilBERT para detecção de ataques DDoS em tempo real usando dados de rede tabulares.\n",
    "\n",
    "**Objetivo**: Criar um modelo leve e eficiente para rodar em Raspberry Pi detectando ataques em tempo real."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Instalação de Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Verificar se estamos no Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import files\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"Executando no Google Colab: {IN_COLAB}\")\n",
    "\n",
    "# Instalar dependências\n",
    "!pip install -q transformers torch onnx onnxruntime numpy pandas scikit-learn matplotlib seaborn optimum[onnxruntime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importar bibliotecas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertModel, DistilBertConfig, Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload e Análise dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Faça upload dos seus arquivos CSV de dados de rede:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Listar arquivos carregados\n",
    "    csv_files = [f for f in uploaded.keys() if f.endswith('.csv')]\n",
    "    print(f\"Arquivos CSV carregados: {csv_files}\")\n",
    "else:\n",
    "    # Para execução local, listar arquivos CSV no diretório\n",
    "    import glob\n",
    "    csv_files = glob.glob(\"*.csv\")\n",
    "    print(f\"Arquivos CSV encontrados: {csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def load_and_analyze_data(csv_files, sample_size=50000):\n",
    "    \"\"\"Carregar e analisar dados de múltiplos arquivos CSV\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for file in csv_files[:5]:  # Limitar a 5 arquivos para teste\n",
    "        print(f\"Carregando {file}...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Amostrar dados para reduzir tempo de processamento\n",
    "            if len(df) > sample_size:\n",
    "                df = df.sample(n=sample_size, random_state=42)\n",
    "            all_data.append(df)\n",
    "            print(f\"  - Shape: {df.shape}\")\n",
    "            print(f\"  - Labels únicos: {df['label'].unique()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Erro ao carregar {file}: {e}\")\n",
    "    \n",
    "    # Combinar todos os dados\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\nDados combinados: {combined_df.shape}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"Nenhum dado foi carregado com sucesso.\")\n",
    "        return None\n",
    "\n",
    "# Carregar dados\n",
    "df = load_and_analyze_data(csv_files)\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\n=== ANÁLISE DOS DADOS ===\")\n",
    "    print(f\"Shape total: {df.shape}\")\n",
    "    print(f\"\\nColunas: {list(df.columns)}\")\n",
    "    print(f\"\\nDistribuição de labels:\")\n",
    "    print(df['label'].value_counts())\n",
    "    print(f\"\\nValores nulos por coluna:\")\n",
    "    print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pré-processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Pré-processar dados para o modelo\"\"\"\n",
    "    \n",
    "    # Remover colunas com muitos valores nulos ou constantes\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remover colunas com mais de 50% de valores nulos\n",
    "    null_threshold = 0.5\n",
    "    null_cols = df_clean.columns[df_clean.isnull().mean() > null_threshold]\n",
    "    df_clean = df_clean.drop(columns=null_cols)\n",
    "    print(f\"Removidas {len(null_cols)} colunas com muitos valores nulos\")\n",
    "    \n",
    "    # Preencher valores nulos restantes\n",
    "    df_clean = df_clean.fillna(0)\n",
    "    \n",
    "    # Separar features e labels\n",
    "    X = df_clean.drop('label', axis=1)\n",
    "    y = df_clean['label']\n",
    "    \n",
    "    # Codificar labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Normalizar features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(f\"\\nFeatures shape: {X_scaled.shape}\")\n",
    "    print(f\"Labels shape: {y_encoded.shape}\")\n",
    "    print(f\"Número de classes: {len(label_encoder.classes_)}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    return X_scaled, y_encoded, label_encoder, scaler, list(X.columns)\n",
    "\n",
    "# Pré-processar dados\n",
    "if df is not None:\n",
    "    X, y, label_encoder, scaler, feature_names = preprocess_data(df)\n",
    "    \n",
    "    # Dividir em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTreino: {X_train.shape}, Teste: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo DistilBERT Adaptado para Dados Tabulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class TabularDistilBERT(nn.Module):\n",
    "    \"\"\"DistilBERT adaptado para dados tabulares de rede\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, num_classes, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Configuração reduzida do DistilBERT para eficiência\n",
    "        config = DistilBertConfig(\n",
    "            vocab_size=1000,  # Reduzido\n",
    "            max_position_embeddings=512,\n",
    "            n_layers=3,  # Reduzido de 6 para 3\n",
    "            n_heads=8,\n",
    "            dim=hidden_dim,  # Reduzido\n",
    "            hidden_dim=hidden_dim * 4,\n",
    "            dropout=0.1,\n",
    "            attention_dropout=0.1\n",
    "        )\n",
    "        \n",
    "        # Camada de projeção para converter features tabulares em embeddings\n",
    "        self.feature_projection = nn.Linear(num_features, hidden_dim)\n",
    "        \n",
    "        # DistilBERT backbone (sem embeddings de palavras)\n",
    "        self.distilbert = DistilBertModel(config)\n",
    "        \n",
    "        # Cabeça de classificação\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Projetar features para dimensão do modelo\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Converter features em embeddings\n",
    "        embeddings = self.feature_projection(features)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Adicionar dimensão de sequência (simular tokens)\n",
    "        embeddings = embeddings.unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "        \n",
    "        # Criar attention mask\n",
    "        attention_mask = torch.ones(batch_size, 1, device=features.device)\n",
    "        \n",
    "        # Passar pelo DistilBERT\n",
    "        outputs = self.distilbert(\n",
    "            inputs_embeds=embeddings,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Usar o último hidden state\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Classificação\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Criar modelo\n",
    "if df is not None:\n",
    "    num_features = X.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    model = TabularDistilBERT(num_features, num_classes, hidden_dim=128)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Modelo criado:\")\n",
    "    print(f\"  - Features de entrada: {num_features}\")\n",
    "    print(f\"  - Classes de saída: {num_classes}\")\n",
    "    print(f\"  - Parâmetros totais: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset e DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class NetworkDataset(Dataset):\n",
    "    \"\"\"Dataset para dados de rede\"\"\"\n",
    "    \n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': self.features[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Criar datasets\n",
    "if df is not None:\n",
    "    train_dataset = NetworkDataset(X_train, y_train)\n",
    "    test_dataset = NetworkDataset(X_test, y_test)\n",
    "    \n",
    "    # Criar dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    print(f\"Dataset de treino: {len(train_dataset)} amostras\")\n",
    "    print(f\"Dataset de teste: {len(test_dataset)} amostras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def train_model(model, train_loader, test_loader, num_epochs=5):\n",
    "    \"\"\"Treinar o modelo\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f\"Época {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "        \n",
    "        # Avaliar no conjunto de teste\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            test_acc = evaluate_model(model, test_loader)\n",
    "            print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Avaliar o modelo\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    model.train()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# Treinar modelo\n",
    "if df is not None:\n",
    "    print(\"Iniciando treinamento...\")\n",
    "    train_losses, train_accuracies = train_model(model, train_loader, test_loader, num_epochs=3)\n",
    "    \n",
    "    # Avaliação final\n",
    "    final_accuracy = evaluate_model(model, test_loader)\n",
    "    print(f\"\\nAccuracy final no teste: {final_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análise Detalhada de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def detailed_evaluation(model, test_loader, label_encoder):\n",
    "    \"\"\"Avaliação detalhada com métricas\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Medir tempo de inferência\n",
    "            start_time = time.time()\n",
    "            outputs = model(features)\n",
    "            inference_time = (time.time() - start_time) * 1000  # ms\n",
    "            inference_times.append(inference_time)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    print(\"\\n=== RELATÓRIO DE PERFORMANCE ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Tempo médio de inferência: {np.mean(inference_times):.2f} ms\")\n",
    "    print(f\"Throughput: {1000/np.mean(inference_times):.2f} predições/segundo\")\n",
    "    \n",
    "    # Relatório de classificação\n",
    "    print(\"\\n=== RELATÓRIO DE CLASSIFICAÇÃO ===\")\n",
    "    print(classification_report(\n",
    "        all_labels, all_predictions, \n",
    "        target_names=label_encoder.classes_,\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    # Matriz de confusão\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, np.mean(inference_times)\n",
    "\n",
    "# Avaliação detalhada\n",
    "if df is not None:\n",
    "    accuracy, avg_inference_time = detailed_evaluation(model, test_loader, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Otimização para Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class OptimizedNetworkDetector(nn.Module):\n",
    "    \"\"\"Wrapper otimizado para exportação ONNX\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, features):\n",
    "        logits = self.model(features)\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        return logits, probabilities\n",
    "\n",
    "def export_optimized_model(model, X_sample, scaler, label_encoder, feature_names):\n",
    "    \"\"\"Exportar modelo otimizado para ONNX\"\"\"\n",
    "    \n",
    "    # Criar wrapper otimizado\n",
    "    optimized_model = OptimizedNetworkDetector(model)\n",
    "    optimized_model.eval()\n",
    "    \n",
    "    # Preparar input de exemplo\n",
    "    dummy_input = torch.FloatTensor(X_sample[:1]).to(device)\n",
    "    \n",
    "    # Exportar para ONNX\n",
    "    torch.onnx.export(\n",
    "        optimized_model,\n",
    "        dummy_input,\n",
    "        'network_attack_detector.onnx',\n",
    "        input_names=['features'],\n",
    "        output_names=['logits', 'probabilities'],\n",
    "        dynamic_axes={\n",
    "            'features': {0: 'batch_size'},\n",
    "            'logits': {0: 'batch_size'},\n",
    "            'probabilities': {0: 'batch_size'}\n",
    "        },\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo exportado para ONNX: network_attack_detector.onnx\")\n",
    "    \n",
    "    # Quantizar modelo\n",
    "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "    \n",
    "    quantize_dynamic(\n",
    "        'network_attack_detector.onnx',\n",
    "        'network_attack_detector_quantized.onnx',\n",
    "        weight_type=QuantType.QInt8\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo quantizado: network_attack_detector_quantized.onnx\")\n",
    "    \n",
    "    # Salvar metadados\n",
    "    import pickle\n",
    "    \n",
    "    metadata = {\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': label_encoder,\n",
    "        'feature_names': feature_names,\n",
    "        'num_features': len(feature_names),\n",
    "        'num_classes': len(label_encoder.classes_),\n",
    "        'classes': label_encoder.classes_.tolist()\n",
    "    }\n",
    "    \n",
    "    with open('model_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    print(\"Metadados salvos: model_metadata.pkl\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Exportar modelo otimizado\n",
    "if df is not None:\n",
    "    print(\"Exportando modelo otimizado...\")\n",
    "    metadata = export_optimized_model(model, X_test, scaler, label_encoder, feature_names)\n",
    "    \n",
    "    # Verificar tamanhos dos arquivos\n",
    "    import os\n",
    "    original_size = os.path.getsize('network_attack_detector.onnx') / (1024*1024)\n",
    "    quantized_size = os.path.getsize('network_attack_detector_quantized.onnx') / (1024*1024)\n",
    "    \n",
    "    print(f\"\\nTamanho do modelo original: {original_size:.2f} MB\")\n",
    "    print(f\"Tamanho do modelo quantizado: {quantized_size:.2f} MB\")\n",
    "    print(f\"Redução: {(1 - quantized_size/original_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sistema de Monitoramento em Tempo Real para Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar script de monitoramento em tempo real\n",
    "monitoring_script = '''\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Sistema de Detecção de Ataques de Rede em Tempo Real\n",
    "Otimizado para Raspberry Pi\n",
    "\"\"\"\n",
    "\n",
    "import onnxruntime as ort\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import argparse\n",
    "import json\n",
    "from datetime import datetime\n",
    "import threading\n",
    "import queue\n",
    "import sys\n",
    "\n",
    "class NetworkAttackDetector:\n",
    "    def __init__(self, model_path, metadata_path):\n",
    "        \"\"\"Inicializar detector de ataques\"\"\"\n",
    "        \n",
    "        print(\"Carregando modelo...\")\n",
    "        self.session = ort.InferenceSession(model_path)\n",
    "        \n",
    "        print(\"Carregando metadados...\")\n",
    "        with open(metadata_path, 'rb') as f:\n",
    "            self.metadata = pickle.load(f)\n",
    "        \n",
    "        self.scaler = self.metadata['scaler']\n",
    "        self.label_encoder = self.metadata['label_encoder']\n",
    "        self.feature_names = self.metadata['feature_names']\n",
    "        self.classes = self.metadata['classes']\n",
    "        \n",
    "        print(f\"Modelo carregado com sucesso!\")\n",
    "        print(f\"Classes detectáveis: {self.classes}\")\n",
    "        \n",
    "        # Estatísticas\n",
    "        self.total_predictions = 0\n",
    "        self.attack_detections = 0\n",
    "        self.inference_times = []\n",
    "    \n",
    "    def preprocess_features(self, features_dict):\n",
    "        \"\"\"Pré-processar features de entrada\"\"\"\n",
    "        \n",
    "        # Converter para array na ordem correta\n",
    "        features_array = np.array([features_dict.get(name, 0.0) for name in self.feature_names])\n",
    "        \n",
    "        # Normalizar\n",
    "        features_scaled = self.scaler.transform(features_array.reshape(1, -1))\n",
    "        \n",
    "        return features_scaled.astype(np.float32)\n",
    "    \n",
    "    def predict(self, features_dict):\n",
    "        \"\"\"Fazer predição de ataque\"\"\"\n",
    "        \n",
    "        # Pré-processar\n",
    "        features = self.preprocess_features(features_dict)\n",
    "        \n",
    "        # Inferência\n",
    "        start_time = time.time()\n",
    "        ort_inputs = {'features': features}\n",
    "        logits, probabilities = self.session.run(None, ort_inputs)\n",
    "        inference_time = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Processar resultado\n",
    "        predicted_class_idx = np.argmax(probabilities[0])\n",
    "        predicted_class = self.classes[predicted_class_idx]\n",
    "        confidence = probabilities[0][predicted_class_idx]\n",
    "        \n",
    "        # Atualizar estatísticas\n",
    "        self.total_predictions += 1\n",
    "        self.inference_times.append(inference_time)\n",
    "        \n",
    "        if predicted_class != 'Benign':  # Assumindo que 'Benign' é tráfego normal\n",
    "            self.attack_detections += 1\n",
    "        \n",
    "        return {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'predicted_class': predicted_class,\n",
    "            'confidence': float(confidence),\n",
    "            'is_attack': predicted_class != 'Benign',\n",
    "            'inference_time_ms': inference_time,\n",
    "            'all_probabilities': probabilities[0].tolist()\n",
    "        }\n",
    "    \n",
    "    def get_statistics(self):\n",
    "        \"\"\"Obter estatísticas do detector\"\"\"\n",
    "        \n",
    "        if not self.inference_times:\n",
    "            return {}\n",
    "        \n",
    "        return {\n",
    "            'total_predictions': self.total_predictions,\n",
    "            'attack_detections': self.attack_detections,\n",
    "            'attack_rate': self.attack_detections / self.total_predictions if self.total_predictions > 0 else 0,\n",
    "            'avg_inference_time_ms': np.mean(self.inference_times),\n",
    "            'max_inference_time_ms': np.max(self.inference_times),\n",
    "            'min_inference_time_ms': np.min(self.inference_times),\n",
    "            'throughput_per_second': 1000 / np.mean(self.inference_times)\n",
    "        }\n",
    "\n",
    "class RealTimeMonitor:\n",
    "    def __init__(self, detector, log_file='attack_log.json'):\n",
    "        self.detector = detector\n",
    "        self.log_file = log_file\n",
    "        self.data_queue = queue.Queue()\n",
    "        self.running = False\n",
    "    \n",
    "    def log_detection(self, result):\n",
    "        \"\"\"Registrar detecção em arquivo\"\"\"\n",
    "        \n",
    "        with open(self.log_file, 'a') as f:\n",
    "            f.write(json.dumps(result) + '\\n')\n",
    "    \n",
    "    def process_data_stream(self):\n",
    "        \"\"\"Processar stream de dados em tempo real\"\"\"\n",
    "        \n",
    "        while self.running:\n",
    "            try:\n",
    "                # Obter dados da fila\n",
    "                features_dict = self.data_queue.get(timeout=1)\n",
    "                \n",
    "                # Fazer predição\n",
    "                result = self.detector.predict(features_dict)\n",
    "                \n",
    "                # Log se for ataque\n",
    "                if result['is_attack']:\n",
    "                    print(f\"🚨 ATAQUE DETECTADO: {result['predicted_class']} (Confiança: {result['confidence']:.3f})\")\n",
    "                    self.log_detection(result)\n",
    "                else:\n",
    "                    print(f\"✅ Tráfego normal (Confiança: {result['confidence']:.3f})\")\n",
    "                \n",
    "                self.data_queue.task_done()\n",
    "                \n",
    "            except queue.Empty:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"Erro no processamento: {e}\")\n",
    "    \n",
    "    def start_monitoring(self):\n",
    "        \"\"\"Iniciar monitoramento\"\"\"\n",
    "        \n",
    "        self.running = True\n",
    "        monitor_thread = threading.Thread(target=self.process_data_stream)\n",
    "        monitor_thread.daemon = True\n",
    "        monitor_thread.start()\n",
    "        \n",
    "        print(\"Monitoramento iniciado...\")\n",
    "        return monitor_thread\n",
    "    \n",
    "    def stop_monitoring(self):\n",
    "        \"\"\"Parar monitoramento\"\"\"\n",
    "        self.running = False\n",
    "    \n",
    "    def add_data(self, features_dict):\n",
    "        \"\"\"Adicionar dados para análise\"\"\"\n",
    "        self.data_queue.put(features_dict)\n",
    "\n",
    "def simulate_network_data(csv_file, detector, monitor, delay=1.0):\n",
    "    \"\"\"Simular dados de rede em tempo real\"\"\"\n",
    "    \n",
    "    print(f\"Carregando dados de simulação: {csv_file}\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    print(f\"Iniciando simulação com {len(df)} amostras...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Converter linha para dicionário (excluindo label)\n",
    "        features_dict = row.drop('label').to_dict()\n",
    "        \n",
    "        # Adicionar à fila de monitoramento\n",
    "        monitor.add_data(features_dict)\n",
    "        \n",
    "        # Mostrar progresso\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            stats = detector.get_statistics()\n",
    "            print(f\"\\nProcessadas {idx + 1} amostras\")\n",
    "            print(f\"Taxa de ataques: {stats.get('attack_rate', 0):.3f}\")\n",
    "            print(f\"Tempo médio: {stats.get('avg_inference_time_ms', 0):.2f} ms\")\n",
    "        \n",
    "        time.sleep(delay)\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='Detector de Ataques de Rede em Tempo Real')\n",
    "    parser.add_argument('--model', default='network_attack_detector_quantized.onnx', help='Modelo ONNX')\n",
    "    parser.add_argument('--metadata', default='model_metadata.pkl', help='Metadados do modelo')\n",
    "    parser.add_argument('--simulate', type=str, help='Arquivo CSV para simulação')\n",
    "    parser.add_argument('--delay', type=float, default=0.1, help='Delay entre amostras (segundos)')\n",
    "    parser.add_argument('--interactive', action='store_true', help='Modo interativo')\n",
    "    parser.add_argument('--benchmark', action='store_true', help='Benchmark de performance')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Inicializar detector\n",
    "    try:\n",
    "        detector = NetworkAttackDetector(args.model, args.metadata)\n",
    "        monitor = RealTimeMonitor(detector)\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao inicializar detector: {e}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    if args.benchmark:\n",
    "        # Benchmark de performance\n",
    "        print(\"Executando benchmark...\")\n",
    "        \n",
    "        # Criar dados de teste\n",
    "        test_features = {name: np.random.randn() for name in detector.feature_names}\n",
    "        \n",
    "        # Executar múltiplas predições\n",
    "        for i in range(1000):\n",
    "            detector.predict(test_features)\n",
    "        \n",
    "        stats = detector.get_statistics()\n",
    "        print(f\"\\nResultados do benchmark:\")\n",
    "        print(f\"Predições: {stats['total_predictions']}\")\n",
    "        print(f\"Tempo médio: {stats['avg_inference_time_ms']:.2f} ms\")\n",
    "        print(f\"Throughput: {stats['throughput_per_second']:.2f} predições/segundo\")\n",
    "        \n",
    "    elif args.simulate:\n",
    "        # Simular dados de rede\n",
    "        monitor_thread = monitor.start_monitoring()\n",
    "        \n",
    "        try:\n",
    "            simulate_network_data(args.simulate, detector, monitor, args.delay)\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nInterrompido pelo usuário\")\n",
    "        finally:\n",
    "            monitor.stop_monitoring()\n",
    "            \n",
    "            # Mostrar estatísticas finais\n",
    "            stats = detector.get_statistics()\n",
    "            print(f\"\\n=== ESTATÍSTICAS FINAIS ===\")\n",
    "            for key, value in stats.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "    \n",
    "    elif args.interactive:\n",
    "        # Modo interativo\n",
    "        print(\"\\nModo interativo ativado.\")\n",
    "        print(\"Digite valores para as features ou 'sair' para encerrar.\")\n",
    "        print(f\"Features necessárias: {detector.feature_names[:5]}... (total: {len(detector.feature_names)})\")\n",
    "        \n",
    "        while True:\n",
    "            try:\n",
    "                # Exemplo simples: usar valores aleatórios\n",
    "                input(\"Pressione Enter para gerar predição com dados aleatórios (ou Ctrl+C para sair): \")\n",
    "                \n",
    "                test_features = {name: np.random.randn() for name in detector.feature_names}\n",
    "                result = detector.predict(test_features)\n",
    "                \n",
    "                print(f\"\\nResultado:\")\n",
    "                print(f\"  Classe: {result['predicted_class']}\")\n",
    "                print(f\"  Confiança: {result['confidence']:.3f}\")\n",
    "                print(f\"  É ataque: {result['is_attack']}\")\n",
    "                print(f\"  Tempo: {result['inference_time_ms']:.2f} ms\")\n",
    "                \n",
    "            except KeyboardInterrupt:\n",
    "                break\n",
    "    \n",
    "    else:\n",
    "        print(\"Use --simulate, --interactive ou --benchmark\")\n",
    "        parser.print_help()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "'''\n",
    "\n",
    "# Salvar script\n",
    "with open('realtime_network_monitor.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(monitoring_script)\n",
    "\n",
    "print(\"Script de monitoramento criado: realtime_network_monitor.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Arquivos de Configuração e Documentação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar requirements.txt\n",
    "requirements = '''onnxruntime==1.15.1\n",
    "numpy==1.24.3\n",
    "pandas==2.0.3\n",
    "scikit-learn==1.3.0\n",
    "'''\n",
    "\n",
    "with open('requirements.txt', 'w') as f:\n",
    "    f.write(requirements)\n",
    "\n",
    "# Criar script de instalação\n",
    "install_script = '''#!/bin/bash\n",
    "# Script de instalação para Raspberry Pi\n",
    "\n",
    "echo \"Instalando dependências para detecção de ataques de rede...\"\n",
    "\n",
    "# Atualizar sistema\n",
    "sudo apt update\n",
    "sudo apt upgrade -y\n",
    "\n",
    "# Instalar Python e pip\n",
    "sudo apt install python3 python3-pip -y\n",
    "\n",
    "# Instalar dependências Python\n",
    "pip3 install -r requirements.txt\n",
    "\n",
    "# Criar diretório de logs\n",
    "mkdir -p logs\n",
    "\n",
    "echo \"Instalação concluída!\"\n",
    "echo \"Para testar: python3 realtime_network_monitor.py --benchmark\"\n",
    "'''\n",
    "\n",
    "with open('install.sh', 'w') as f:\n",
    "    f.write(install_script)\n",
    "\n",
    "# Criar README detalhado\n",
    "readme = f'''# Sistema de Detecção de Ataques de Rede em Tempo Real\n",
    "\n",
    "Sistema baseado em DistilBERT otimizado para detectar ataques DDoS em tempo real em dispositivos IoT como Raspberry Pi.\n",
    "\n",
    "## Características do Modelo\n",
    "\n",
    "- **Modelo**: DistilBERT adaptado para dados tabulares\n",
    "- **Otimizações**: Quantização INT8, ONNX Runtime\n",
    "- **Tamanho**: ~{quantized_size:.1f} MB (redução de {(1 - quantized_size/original_size)*100:.1f}%)\n",
    "- **Performance**: ~{avg_inference_time:.1f} ms por predição\n",
    "- **Classes detectáveis**: {len(label_encoder.classes_)} tipos de ataques\n",
    "\n",
    "## Tipos de Ataques Detectados\n",
    "\n",
    "{chr(10).join([f\"- {cls}\" for cls in label_encoder.classes_])}\n",
    "\n",
    "## Instalação no Raspberry Pi\n",
    "\n",
    "1. Transfira todos os arquivos para o Raspberry Pi\n",
    "2. Execute o script de instalação:\n",
    "```bash\n",
    "chmod +x install.sh\n",
    "./install.sh\n",
    "```\n",
    "\n",
    "## Uso\n",
    "\n",
    "### Benchmark de Performance\n",
    "```bash\n",
    "python3 realtime_network_monitor.py --benchmark\n",
    "```\n",
    "\n",
    "### Simulação com Dados CSV\n",
    "```bash\n",
    "python3 realtime_network_monitor.py --simulate dados_rede.csv --delay 0.1\n",
    "```\n",
    "\n",
    "### Modo Interativo\n",
    "```bash\n",
    "python3 realtime_network_monitor.py --interactive\n",
    "```\n",
    "\n",
    "## Arquivos Incluídos\n",
    "\n",
    "- `network_attack_detector_quantized.onnx`: Modelo otimizado\n",
    "- `model_metadata.pkl`: Metadados (scaler, encoder, features)\n",
    "- `realtime_network_monitor.py`: Sistema de monitoramento\n",
    "- `requirements.txt`: Dependências Python\n",
    "- `install.sh`: Script de instalação\n",
    "\n",
    "## Métricas de Performance\n",
    "\n",
    "- **Accuracy**: {accuracy:.3f}\n",
    "- **Tempo de Inferência**: {avg_inference_time:.2f} ms\n",
    "- **Throughput**: {1000/avg_inference_time:.1f} predições/segundo\n",
    "- **Uso de Memória**: < 100 MB\n",
    "\n",
    "## Monitoramento em Tempo Real\n",
    "\n",
    "O sistema pode processar dados de rede em tempo real e:\n",
    "- Detectar ataques com alta precisão\n",
    "- Registrar detecções em arquivo JSON\n",
    "- Mostrar estatísticas em tempo real\n",
    "- Funcionar 24/7 em Raspberry Pi\n",
    "\n",
    "## Integração com Sistemas de Rede\n",
    "\n",
    "Para integrar com sistemas reais de monitoramento de rede:\n",
    "1. Capture pacotes com ferramentas como tcpdump/Wireshark\n",
    "2. Extraia features de rede (duração, flags, protocolos, etc.)\n",
    "3. Alimente o detector em tempo real\n",
    "4. Configure alertas para ataques detectados\n",
    "\n",
    "## Suporte\n",
    "\n",
    "Para dúvidas ou problemas, verifique:\n",
    "- Logs do sistema em `attack_log.json`\n",
    "- Compatibilidade das features de entrada\n",
    "- Versões das dependências\n",
    "'''\n",
    "\n",
    "with open('README.md', 'w', encoding='utf-8') as f:\n",
    "    f.write(readme)\n",
    "\n",
    "print(\"Documentação criada!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download dos Arquivos para Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "if IN_COLAB and df is not None:\n",
    "    print(\"Preparando arquivos para download...\")\n",
    "    \n",
    "    # Lista de arquivos para download\n",
    "    files_to_download = [\n",
    "        'network_attack_detector_quantized.onnx',\n",
    "        'model_metadata.pkl',\n",
    "        'realtime_network_monitor.py',\n",
    "        'requirements.txt',\n",
    "        'install.sh',\n",
    "        'README.md'\n",
    "    ]\n",
    "    \n",
    "    # Download dos arquivos\n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            files.download(file)\n",
    "            print(f\"✅ {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao baixar {file}: {e}\")\n",
    "    \n",
    "    print(\"\\n=== RESUMO DO SISTEMA ===\")\n",
    "    print(f\"📊 Accuracy do modelo: {accuracy:.3f}\")\n",
    "    print(f\"⚡ Tempo de inferência: {avg_inference_time:.2f} ms\")\n",
    "    print(f\"🚀 Throughput: {1000/avg_inference_time:.1f} predições/segundo\")\n",
    "    print(f\"💾 Tamanho do modelo: {quantized_size:.1f} MB\")\n",
    "    print(f\"🎯 Classes detectáveis: {len(label_encoder.classes_)}\")\n",
    "    \n",
    "    print(\"\\n=== PRÓXIMOS PASSOS ===\")\n",
    "    print(\"1. Transfira todos os arquivos para o Raspberry Pi\")\n",
    "    print(\"2. Execute: chmod +x install.sh && ./install.sh\")\n",
    "    print(\"3. Teste: python3 realtime_network_monitor.py --benchmark\")\n",
    "    print(\"4. Use com seus dados: python3 realtime_network_monitor.py --simulate seu_arquivo.csv\")\n",
    "    \n",
    "elif df is not None:\n",
    "    print(\"\\nArquivos salvos localmente:\")\n",
    "    print(\"- network_attack_detector_quantized.onnx\")\n",
    "    print(\"- model_metadata.pkl\")\n",
    "    print(\"- realtime_network_monitor.py\")\n",
    "    print(\"- requirements.txt\")\n",
    "    print(\"- install.sh\")\n",
    "    print(\"- README.md\")\n",
    "else:\n",
    "    print(\"⚠️ Nenhum dado foi carregado. Faça upload dos arquivos CSV primeiro.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Detecção de Ataques de Rede - DistilBERT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}