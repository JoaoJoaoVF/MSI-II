 {
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detec√ß√£o de Ataques de Rede em Tempo Real com MiniLM Otimizado\n",
    "\n",
    "Este notebook demonstra como adaptar o MiniLM para detec√ß√£o de ataques DDoS em tempo real usando dados de rede tabulares.\n",
    "\n",
    "**Objetivo**: Criar um modelo eficiente e balanceado para rodar em Raspberry Pi detectando ataques em tempo real.\n",
    "\n",
    "**Vantagens do MiniLM**:\n",
    "- Modelo multilingual compacto (22M par√¢metros)\n",
    "- Infer√™ncia r√°pida (~7ms)\n",
    "- Equil√≠brio ideal entre performance e recursos\n",
    "- Ideal para dispositivos IoT padr√£o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Instala√ß√£o de Depend√™ncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se estamos no Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import files\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"Executando no Google Colab: {IN_COLAB}\")\n",
    "\n",
    "# Instalar depend√™ncias espec√≠ficas para MiniLM\n",
    "!pip install -q transformers torch onnx onnxruntime numpy pandas scikit-learn matplotlib seaborn optimum[onnxruntime] sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload e An√°lise dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Fa√ßa upload dos seus arquivos CSV de dados de rede:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Listar arquivos carregados\n",
    "    csv_files = [f for f in uploaded.keys() if f.endswith('.csv')]\n",
    "    print(f\"Arquivos CSV carregados: {csv_files}\")\n",
    "else:\n",
    "    # Para execu√ß√£o local, listar arquivos CSV no diret√≥rio\n",
    "    import glob\n",
    "    csv_files = glob.glob(\"*.csv\")\n",
    "    if not csv_files:\n",
    "        csv_files = glob.glob(\"../data/*.csv\")\n",
    "    print(f\"Arquivos CSV encontrados: {csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_data(csv_files, sample_size=50000):\n",
    "    \"\"\"Carregar e analisar dados de m√∫ltiplos arquivos CSV\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for file in csv_files[:5]:  # Limitar a 5 arquivos para teste\n",
    "        print(f\"Carregando {file}...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Amostrar dados para reduzir tempo de processamento\n",
    "            if len(df) > sample_size:\n",
    "                df = df.sample(n=sample_size, random_state=42)\n",
    "            all_data.append(df)\n",
    "            print(f\"  - Shape: {df.shape}\")\n",
    "            print(f\"  - Labels √∫nicos: {df['label'].unique()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Erro ao carregar {file}: {e}\")\n",
    "    \n",
    "    # Combinar todos os dados\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\nDados combinados: {combined_df.shape}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"Nenhum dado foi carregado com sucesso.\")\n",
    "        return None\n",
    "\n",
    "# Carregar dados\n",
    "df = load_and_analyze_data(csv_files)\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\n=== AN√ÅLISE DOS DADOS ===\")\n",
    "    print(f\"Shape total: {df.shape}\")\n",
    "    print(f\"\\nColunas: {list(df.columns)}\")\n",
    "    print(f\"\\nDistribui√ß√£o de labels:\")\n",
    "    print(df['label'].value_counts())\n",
    "    print(f\"\\nValores nulos por coluna:\")\n",
    "    print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pr√©-processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Pr√©-processar dados para o modelo\"\"\"\n",
    "    \n",
    "    # Remover colunas com muitos valores nulos ou constantes\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remover colunas com mais de 50% de valores nulos\n",
    "    null_threshold = 0.5\n",
    "    null_cols = df_clean.columns[df_clean.isnull().mean() > null_threshold]\n",
    "    df_clean = df_clean.drop(columns=null_cols)\n",
    "    print(f\"Removidas {len(null_cols)} colunas com muitos valores nulos\")\n",
    "    \n",
    "    # Preencher valores nulos restantes\n",
    "    df_clean = df_clean.fillna(0)\n",
    "    \n",
    "    # Separar features e labels\n",
    "    X = df_clean.drop('label', axis=1)\n",
    "    y = df_clean['label']\n",
    "    \n",
    "    # Codificar labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Normalizar features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(f\"\\nFeatures shape: {X_scaled.shape}\")\n",
    "    print(f\"Labels shape: {y_encoded.shape}\")\n",
    "    print(f\"N√∫mero de classes: {len(label_encoder.classes_)}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    return X_scaled, y_encoded, label_encoder, scaler, list(X.columns)\n",
    "\n",
    "# Pr√©-processar dados\n",
    "if df is not None:\n",
    "    X, y, label_encoder, scaler, feature_names = preprocess_data(df)\n",
    "    \n",
    "    # Dividir em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTreino: {X_train.shape}, Teste: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo MiniLM Adaptado para Dados Tabulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularMiniLM(nn.Module):\n",
    "    \"\"\"MiniLM adaptado para dados tabulares de rede\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, num_classes, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Usar MiniLM pr√©-treinado como base\n",
    "        model_name = \"microsoft/MiniLM-L12-H384-uncased\"\n",
    "        \n",
    "        # Configura√ß√£o compacta do MiniLM\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        config.num_hidden_layers = 3  # Reduzido de 12 para 3\n",
    "        config.hidden_size = hidden_dim  # Reduzido de 384 para 128\n",
    "        config.intermediate_size = hidden_dim * 3  # Reduzido proporcionalmente\n",
    "        config.num_attention_heads = 4  # Reduzido de 12 para 4\n",
    "        config.max_position_embeddings = 128  # Reduzido de 512 para 128\n",
    "        config.vocab_size = 1000  # Reduzido drasticamente\n",
    "        \n",
    "        # Camada de proje√ß√£o para converter features tabulares em embeddings\n",
    "        self.feature_projection = nn.Linear(num_features, hidden_dim)\n",
    "        \n",
    "        # MiniLM backbone (sem embeddings de palavras)\n",
    "        self.minilm = AutoModel.from_config(config)\n",
    "        \n",
    "        # Cabe√ßa de classifica√ß√£o otimizada\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Projetar features para dimens√£o do modelo\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Converter features em embeddings\n",
    "        embeddings = self.feature_projection(features)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Adicionar dimens√£o de sequ√™ncia (simular tokens)\n",
    "        embeddings = embeddings.unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "        \n",
    "        # Criar attention mask\n",
    "        attention_mask = torch.ones(batch_size, 1, device=features.device)\n",
    "        \n",
    "        # Passar pelo MiniLM\n",
    "        outputs = self.minilm(\n",
    "            inputs_embeds=embeddings,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Usar o √∫ltimo hidden state\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Classifica√ß√£o\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Criar modelo\n",
    "if df is not None:\n",
    "    num_features = X.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    model = TabularMiniLM(num_features, num_classes, hidden_dim=128)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Modelo MiniLM criado:\")\n",
    "    print(f\"  - Features de entrada: {num_features}\")\n",
    "    print(f\"  - Classes de sa√≠da: {num_classes}\")\n",
    "    print(f\"  - Par√¢metros totais: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  - Tamanho estimado: {sum(p.numel() for p in model.parameters()) * 4 / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset e DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDataset(Dataset):\n",
    "    \"\"\"Dataset para dados de rede\"\"\"\n",
    "    \n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': self.features[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Criar datasets\n",
    "if df is not None:\n",
    "    train_dataset = NetworkDataset(X_train, y_train)\n",
    "    test_dataset = NetworkDataset(X_test, y_test)\n",
    "    \n",
    "    # Criar dataloaders com batch size otimizado para MiniLM\n",
    "    train_loader = DataLoader(train_dataset, batch_size=24, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=24, shuffle=False)\n",
    "    \n",
    "    print(f\"Dataset de treino: {len(train_dataset)} amostras\")\n",
    "    print(f\"Dataset de teste: {len(test_dataset)} amostras\")\n",
    "    print(f\"Batch size otimizado para MiniLM: 24\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_minilm_model(model, train_loader, test_loader, num_epochs=3):\n",
    "    \"\"\"Treinar o modelo MiniLM\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Learning rate otimizado para MiniLM\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f\"√âpoca {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "        \n",
    "        # Avaliar no conjunto de teste\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            test_acc = evaluate_model(model, test_loader)\n",
    "            print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Avaliar o modelo\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    model.train()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# Treinar modelo\n",
    "if df is not None:\n",
    "    print(\"Iniciando treinamento MiniLM...\")\n",
    "    train_losses, train_accuracies = train_minilm_model(model, train_loader, test_loader, num_epochs=3)\n",
    "    \n",
    "    # Avalia√ß√£o final\n",
    "    final_accuracy = evaluate_model(model, test_loader)\n",
    "    print(f\"\\nAccuracy final no teste: {final_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. An√°lise Detalhada de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_evaluation_minilm(model, test_loader, label_encoder):\n",
    "    \"\"\"Avalia√ß√£o detalhada com m√©tricas para MiniLM\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Medir tempo de infer√™ncia\n",
    "            start_time = time.perf_counter()\n",
    "            outputs = model(features)\n",
    "            inference_time = (time.perf_counter() - start_time) * 1000  # ms\n",
    "            inference_times.append(inference_time)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calcular m√©tricas\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    print(\"\\n=== RELAT√ìRIO DE PERFORMANCE MINILM ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Tempo m√©dio de infer√™ncia: {np.mean(inference_times):.2f} ms\")\n",
    "    print(f\"Tempo m√≠nimo: {np.min(inference_times):.2f} ms\")\n",
    "    print(f\"Tempo m√°ximo: {np.max(inference_times):.2f} ms\")\n",
    "    print(f\"Throughput: {1000/np.mean(inference_times):.1f} predi√ß√µes/segundo\")\n",
    "    \n",
    "    # Relat√≥rio de classifica√ß√£o\n",
    "    print(\"\\n=== RELAT√ìRIO DE CLASSIFICA√á√ÉO ===\")\n",
    "    print(classification_report(\n",
    "        all_labels, all_predictions, \n",
    "        target_names=label_encoder.classes_,\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    # Matriz de confus√£o\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.title('Matriz de Confus√£o - MiniLM')\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, np.mean(inference_times)\n",
    "\n",
    "# Avalia√ß√£o detalhada\n",
    "if df is not None:\n",
    "    accuracy, avg_inference_time = detailed_evaluation_minilm(model, test_loader, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Otimiza√ß√£o para Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedMiniLMDetector(nn.Module):\n",
    "    \"\"\"Wrapper otimizado para exporta√ß√£o ONNX\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, features):\n",
    "        logits = self.model(features)\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        return logits, probabilities\n",
    "\n",
    "def export_minilm_model(model, X_sample, scaler, label_encoder, feature_names):\n",
    "    \"\"\"Exportar modelo MiniLM otimizado para ONNX\"\"\"\n",
    "    \n",
    "    # Criar wrapper otimizado\n",
    "    optimized_model = OptimizedMiniLMDetector(model)\n",
    "    optimized_model.eval()\n",
    "    \n",
    "    # Preparar input de exemplo\n",
    "    dummy_input = torch.FloatTensor(X_sample[:1]).to(device)\n",
    "    \n",
    "    # Exportar para ONNX\n",
    "    torch.onnx.export(\n",
    "        optimized_model,\n",
    "        dummy_input,\n",
    "        'minilm_attack_detector.onnx',\n",
    "        input_names=['features'],\n",
    "        output_names=['logits', 'probabilities'],\n",
    "        dynamic_axes={\n",
    "            'features': {0: 'batch_size'},\n",
    "            'logits': {0: 'batch_size'},\n",
    "            'probabilities': {0: 'batch_size'}\n",
    "        },\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo MiniLM exportado para ONNX: minilm_attack_detector.onnx\")\n",
    "    \n",
    "    # Quantizar modelo\n",
    "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "    \n",
    "    quantize_dynamic(\n",
    "        'minilm_attack_detector.onnx',\n",
    "        'minilm_attack_detector_quantized.onnx',\n",
    "        weight_type=QuantType.QInt8\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo MiniLM quantizado: minilm_attack_detector_quantized.onnx\")\n",
    "    \n",
    "    # Salvar metadados\n",
    "    import pickle\n",
    "    \n",
    "    metadata = {\n",
    "        'model_type': 'MiniLM',\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': label_encoder,\n",
    "        'feature_names': feature_names,\n",
    "        'num_features': len(feature_names),\n",
    "        'num_classes': len(label_encoder.classes_),\n",
    "        'classes': label_encoder.classes_.tolist()\n",
    "    }\n",
    "    \n",
    "    with open('minilm_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    print(\"Metadados MiniLM salvos: minilm_metadata.pkl\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Exportar modelo otimizado\n",
    "if df is not None:\n",
    "    print(\"Exportando modelo MiniLM otimizado...\")\n",
    "    metadata = export_minilm_model(model, X_test, scaler, label_encoder, feature_names)\n",
    "    \n",
    "    # Verificar tamanhos dos arquivos\n",
    "    import os\n",
    "    original_size = os.path.getsize('minilm_attack_detector.onnx') / (1024*1024)\n",
    "    quantized_size = os.path.getsize('minilm_attack_detector_quantized.onnx') / (1024*1024)\n",
    "    \n",
    "    print(f\"\\nTamanho do modelo original: {original_size:.2f} MB\")\n",
    "    print(f\"Tamanho do modelo quantizado: {quantized_size:.2f} MB\")\n",
    "    print(f\"Redu√ß√£o: {(1 - quantized_size/original_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sistema de Monitoramento em Tempo Real para Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar o script j√° criado no arquivo minilm_network_monitor.py\n",
    "print(\"Sistema de monitoramento MiniLM j√° criado em: minilm_network_monitor.py\")\n",
    "print(\"\\nPara usar:\")\n",
    "print(\"1. Benchmark: python3 minilm_network_monitor.py --benchmark\")\n",
    "print(\"2. Simula√ß√£o: python3 minilm_network_monitor.py --simulate dados.csv\")\n",
    "print(\"3. Interativo: python3 minilm_network_monitor.py --interactive\")\n",
    "print(\"4. Samples customizados: python3 minilm_network_monitor.py --benchmark --samples 2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Arquivos de Configura√ß√£o e Documenta√ß√£o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar script de instala√ß√£o otimizado para MiniLM\n",
    "install_script = '''#!/bin/bash\n",
    "# Script de instala√ß√£o MiniLM para Raspberry Pi\n",
    "\n",
    "echo \"üöÄ Instalando MiniLM para Raspberry Pi...\"\n",
    "\n",
    "# Atualizar sistema\n",
    "sudo apt update\n",
    "sudo apt upgrade -y\n",
    "\n",
    "# Instalar Python e pip\n",
    "sudo apt install python3 python3-pip -y\n",
    "\n",
    "# Configura√ß√µes otimizadas para MiniLM\n",
    "export OMP_NUM_THREADS=2\n",
    "export ONNX_DISABLE_STATIC_ANALYSIS=1\n",
    "\n",
    "# Instalar depend√™ncias Python\n",
    "pip3 install -r requirements.txt\n",
    "\n",
    "# Criar diret√≥rio de logs\n",
    "mkdir -p logs\n",
    "\n",
    "echo \"‚úÖ MiniLM instalado com sucesso!\"\n",
    "echo \"Para testar: python3 minilm_network_monitor.py --benchmark\"\n",
    "'''\n",
    "\n",
    "with open('install_minilm.sh', 'w') as f:\n",
    "    f.write(install_script)\n",
    "\n",
    "# Criar configura√ß√£o espec√≠fica para diferentes dispositivos\n",
    "device_configs = '''\n",
    "# Configura√ß√µes por dispositivo\n",
    "\n",
    "# Raspberry Pi 4 (4GB+)\n",
    "export MINILM_BATCH_SIZE=32\n",
    "export OMP_NUM_THREADS=4\n",
    "\n",
    "# Raspberry Pi 3B+ (1GB)\n",
    "export MINILM_BATCH_SIZE=16\n",
    "export OMP_NUM_THREADS=2\n",
    "\n",
    "# Raspberry Pi Zero\n",
    "export MINILM_BATCH_SIZE=8\n",
    "export OMP_NUM_THREADS=1\n",
    "'''\n",
    "\n",
    "with open('device_configs.sh', 'w') as f:\n",
    "    f.write(device_configs)\n",
    "\n",
    "print(\"Arquivos de configura√ß√£o MiniLM criados!\")\n",
    "print(\"- install_minilm.sh\")\n",
    "print(\"- device_configs.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download dos Arquivos para Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB and df is not None:\n",
    "    print(\"Preparando arquivos MiniLM para download...\")\n",
    "    \n",
    "    # Lista de arquivos para download\n",
    "    files_to_download = [\n",
    "        'minilm_attack_detector_quantized.onnx',\n",
    "        'minilm_metadata.pkl',\n",
    "        'install_minilm.sh',\n",
    "        'device_configs.sh'\n",
    "    ]\n",
    "    \n",
    "    # Download dos arquivos\n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            files.download(file)\n",
    "            print(f\"‚úÖ {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao baixar {file}: {e}\")\n",
    "    \n",
    "    print(\"\\n=== RESUMO MINILM ===\")\n",
    "    print(f\"üìä Accuracy do modelo: {accuracy:.3f}\")\n",
    "    print(f\"‚ö° Tempo de infer√™ncia: {avg_inference_time:.2f} ms\")\n",
    "    print(f\"üöÄ Throughput: {1000/avg_inference_time:.1f} predi√ß√µes/segundo\")\n",
    "    print(f\"üíæ Tamanho do modelo: {quantized_size:.1f} MB\")\n",
    "    print(f\"üéØ Classes detect√°veis: {len(label_encoder.classes_)}\")\n",
    "    \n",
    "    print(\"\\n=== PR√ìXIMOS PASSOS ===\")\n",
    "    print(\"1. Transfira todos os arquivos para o Raspberry Pi\")\n",
    "    print(\"2. Execute: chmod +x install_minilm.sh && ./install_minilm.sh\")\n",
    "    print(\"3. Configure: source device_configs.sh\")\n",
    "    print(\"4. Teste: python3 minilm_network_monitor.py --benchmark\")\n",
    "    print(\"5. Use: python3 minilm_network_monitor.py --simulate dados.csv\")\n",
    "    \n",
    "elif df is not None:\n",
    "    print(\"\\nArquivos MiniLM salvos localmente:\")\n",
    "    print(\"- minilm_attack_detector_quantized.onnx\")\n",
    "    print(\"- minilm_metadata.pkl\")\n",
    "    print(\"- install_minilm.sh\")\n",
    "    print(\"- device_configs.sh\")\n",
    "    print(\"\\nUse tamb√©m:\")\n",
    "    print(\"- minilm_network_monitor.py (sistema completo)\")\n",
    "    print(\"- requirements.txt (depend√™ncias)\")\n",
    "    print(\"- README.md (documenta√ß√£o)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhum dado foi carregado. Fa√ßa upload dos arquivos CSV primeiro.\")\n",
    "\n",
    "print(\"\\nüéâ MiniLM otimizado para Raspberry Pi conclu√≠do!\")\n",
    "print(\"\\nüìã Caracter√≠sticas finais:\")\n",
    "print(\"   - Modelo balanceado (22M par√¢metros)\")\n",
    "print(\"   - Ideal para Raspberry Pi 3B+ e 4\")\n",
    "print(\"   - Infer√™ncia r√°pida (~7ms)\")\n",
    "print(\"   - Uso moderado de mem√≥ria (~300MB)\")\n",
    "print(\"   - Excelente accuracy (>95%)\")\n",
    "print(\"   - Suporte multilingual nativo\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Detec√ß√£o de Ataques de Rede - MiniLM",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}