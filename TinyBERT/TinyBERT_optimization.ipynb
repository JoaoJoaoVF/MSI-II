{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detecção de Ataques de Rede em Tempo Real com TinyBERT Otimizado\n",
    "\n",
    "Este notebook demonstra como adaptar o TinyBERT para detecção de ataques DDoS em tempo real usando dados de rede tabulares.\n",
    "\n",
    "**Objetivo**: Criar um modelo extremamente leve e eficiente para rodar em Raspberry Pi e dispositivos IoT detectando ataques em tempo real.\n",
    "\n",
    "**Vantagens do TinyBERT**:\n",
    "- Menor modelo da família BERT (14.5M parâmetros)\n",
    "- Inferência ultra-rápida (<5ms)\n",
    "- Uso mínimo de memória (<200MB)\n",
    "- Ideal para dispositivos IoT extremos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Instalação de Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar se estamos no Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    from google.colab import files\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "print(f\"Executando no Google Colab: {IN_COLAB}\")\n",
    "\n",
    "# Instalar dependências específicas para TinyBERT\n",
    "!pip install -q transformers torch onnx onnxruntime numpy pandas scikit-learn matplotlib seaborn optimum[onnxruntime]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar bibliotecas\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertConfig, AutoModel, AutoConfig\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Upload e Análise dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    print(\"Faça upload dos seus arquivos CSV de dados de rede:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Listar arquivos carregados\n",
    "    csv_files = [f for f in uploaded.keys() if f.endswith('.csv')]\n",
    "    print(f\"Arquivos CSV carregados: {csv_files}\")\n",
    "else:\n",
    "    # Para execução local, listar arquivos CSV no diretório\n",
    "    import glob\n",
    "    csv_files = glob.glob(\"*.csv\")\n",
    "    if not csv_files:\n",
    "        csv_files = glob.glob(\"../data/*.csv\")\n",
    "    print(f\"Arquivos CSV encontrados: {csv_files}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_analyze_data(csv_files, sample_size=50000):\n",
    "    \"\"\"Carregar e analisar dados de múltiplos arquivos CSV\"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for file in csv_files[:5]:  # Limitar a 5 arquivos para teste\n",
    "        print(f\"Carregando {file}...\")\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            # Amostrar dados para reduzir tempo de processamento\n",
    "            if len(df) > sample_size:\n",
    "                df = df.sample(n=sample_size, random_state=42)\n",
    "            all_data.append(df)\n",
    "            print(f\"  - Shape: {df.shape}\")\n",
    "            print(f\"  - Labels únicos: {df['label'].unique()}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  - Erro ao carregar {file}: {e}\")\n",
    "    \n",
    "    # Combinar todos os dados\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        print(f\"\\nDados combinados: {combined_df.shape}\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"Nenhum dado foi carregado com sucesso.\")\n",
    "        return None\n",
    "\n",
    "# Carregar dados\n",
    "df = load_and_analyze_data(csv_files)\n",
    "\n",
    "if df is not None:\n",
    "    print(\"\\n=== ANÁLISE DOS DADOS ===\")\n",
    "    print(f\"Shape total: {df.shape}\")\n",
    "    print(f\"\\nColunas: {list(df.columns)}\")\n",
    "    print(f\"\\nDistribuição de labels:\")\n",
    "    print(df['label'].value_counts())\n",
    "    print(f\"\\nValores nulos por coluna:\")\n",
    "    print(df.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pré-processamento dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Pré-processar dados para o modelo\"\"\"\n",
    "    \n",
    "    # Remover colunas com muitos valores nulos ou constantes\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Remover colunas com mais de 50% de valores nulos\n",
    "    null_threshold = 0.5\n",
    "    null_cols = df_clean.columns[df_clean.isnull().mean() > null_threshold]\n",
    "    df_clean = df_clean.drop(columns=null_cols)\n",
    "    print(f\"Removidas {len(null_cols)} colunas com muitos valores nulos\")\n",
    "    \n",
    "    # Preencher valores nulos restantes\n",
    "    df_clean = df_clean.fillna(0)\n",
    "    \n",
    "    # Separar features e labels\n",
    "    X = df_clean.drop('label', axis=1)\n",
    "    y = df_clean['label']\n",
    "    \n",
    "    # Codificar labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y)\n",
    "    \n",
    "    # Normalizar features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(f\"\\nFeatures shape: {X_scaled.shape}\")\n",
    "    print(f\"Labels shape: {y_encoded.shape}\")\n",
    "    print(f\"Número de classes: {len(label_encoder.classes_)}\")\n",
    "    print(f\"Classes: {label_encoder.classes_}\")\n",
    "    \n",
    "    return X_scaled, y_encoded, label_encoder, scaler, list(X.columns)\n",
    "\n",
    "# Pré-processar dados\n",
    "if df is not None:\n",
    "    X, y, label_encoder, scaler, feature_names = preprocess_data(df)\n",
    "    \n",
    "    # Dividir em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTreino: {X_train.shape}, Teste: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modelo TinyBERT Adaptado para Dados Tabulares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularTinyBERT(nn.Module):\n",
    "    \"\"\"TinyBERT adaptado para dados tabulares de rede\"\"\"\n",
    "    \n",
    "    def __init__(self, num_features, num_classes, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Configuração ultra-compacta do TinyBERT\n",
    "        config = BertConfig(\n",
    "            vocab_size=500,  # Extremamente reduzido\n",
    "            hidden_size=hidden_dim,  # 64 (vs 768 do BERT original)\n",
    "            num_hidden_layers=2,  # 2 (vs 12 do BERT original)\n",
    "            num_attention_heads=2,  # 2 (vs 12 do BERT original)\n",
    "            intermediate_size=hidden_dim * 2,  # 128\n",
    "            max_position_embeddings=32,  # Muito reduzido\n",
    "            type_vocab_size=2,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1\n",
    "        )\n",
    "        \n",
    "        # Camada de projeção para converter features tabulares em embeddings\n",
    "        self.feature_projection = nn.Linear(num_features, hidden_dim)\n",
    "        \n",
    "        # TinyBERT backbone (sem embeddings de palavras)\n",
    "        self.tinybert = BertModel(config)\n",
    "        \n",
    "        # Cabeça de classificação minimalista\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim // 2, num_classes)\n",
    "        )\n",
    "        \n",
    "        self.num_features = num_features\n",
    "        self.num_classes = num_classes\n",
    "    \n",
    "    def forward(self, features):\n",
    "        # Projetar features para dimensão do modelo\n",
    "        batch_size = features.shape[0]\n",
    "        \n",
    "        # Converter features em embeddings\n",
    "        embeddings = self.feature_projection(features)  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Adicionar dimensão de sequência (simular tokens)\n",
    "        embeddings = embeddings.unsqueeze(1)  # [batch_size, 1, hidden_dim]\n",
    "        \n",
    "        # Criar attention mask\n",
    "        attention_mask = torch.ones(batch_size, 1, device=features.device)\n",
    "        \n",
    "        # Passar pelo TinyBERT\n",
    "        outputs = self.tinybert(\n",
    "            inputs_embeds=embeddings,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        \n",
    "        # Usar o último hidden state\n",
    "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [batch_size, hidden_dim]\n",
    "        \n",
    "        # Classificação\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        return logits\n",
    "\n",
    "# Criar modelo\n",
    "if df is not None:\n",
    "    num_features = X.shape[1]\n",
    "    num_classes = len(label_encoder.classes_)\n",
    "    \n",
    "    model = TabularTinyBERT(num_features, num_classes, hidden_dim=64)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    print(f\"Modelo TinyBERT criado:\")\n",
    "    print(f\"  - Features de entrada: {num_features}\")\n",
    "    print(f\"  - Classes de saída: {num_classes}\")\n",
    "    print(f\"  - Parâmetros totais: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(f\"  - Tamanho estimado: {sum(p.numel() for p in model.parameters()) * 4 / (1024*1024):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Dataset e DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkDataset(Dataset):\n",
    "    \"\"\"Dataset para dados de rede\"\"\"\n",
    "    \n",
    "    def __init__(self, features, labels):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'features': self.features[idx],\n",
    "            'labels': self.labels[idx]\n",
    "        }\n",
    "\n",
    "# Criar datasets\n",
    "if df is not None:\n",
    "    train_dataset = NetworkDataset(X_train, y_train)\n",
    "    test_dataset = NetworkDataset(X_test, y_test)\n",
    "    \n",
    "    # Criar dataloaders com batch size menor para TinyBERT\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "    \n",
    "    print(f\"Dataset de treino: {len(train_dataset)} amostras\")\n",
    "    print(f\"Dataset de teste: {len(test_dataset)} amostras\")\n",
    "    print(f\"Batch size otimizado para TinyBERT: 16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Treinamento do Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tinybert_model(model, train_loader, test_loader, num_epochs=3):\n",
    "    \"\"\"Treinar o modelo TinyBERT\"\"\"\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Learning rate maior para TinyBERT devido ao tamanho menor\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(features)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_loss = total_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        \n",
    "        train_losses.append(epoch_loss)\n",
    "        train_accuracies.append(epoch_acc)\n",
    "        \n",
    "        print(f\"Época {epoch+1}/{num_epochs}:\")\n",
    "        print(f\"  Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "        \n",
    "        # Avaliar no conjunto de teste\n",
    "        if (epoch + 1) % 1 == 0:\n",
    "            test_acc = evaluate_model(model, test_loader)\n",
    "            print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
    "    \n",
    "    return train_losses, train_accuracies\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Avaliar o modelo\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(features)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    model.train()\n",
    "    return 100 * correct / total\n",
    "\n",
    "# Treinar modelo\n",
    "if df is not None:\n",
    "    print(\"Iniciando treinamento TinyBERT...\")\n",
    "    train_losses, train_accuracies = train_tinybert_model(model, train_loader, test_loader, num_epochs=3)\n",
    "    \n",
    "    # Avaliação final\n",
    "    final_accuracy = evaluate_model(model, test_loader)\n",
    "    print(f\"\\nAccuracy final no teste: {final_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Análise Detalhada de Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_evaluation_tinybert(model, test_loader, label_encoder):\n",
    "    \"\"\"Avaliação detalhada com métricas para TinyBERT\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            features = batch['features'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Medir tempo de inferência ultra-preciso\n",
    "            start_time = time.perf_counter()\n",
    "            outputs = model(features)\n",
    "            inference_time = (time.perf_counter() - start_time) * 1000  # ms\n",
    "            inference_times.append(inference_time)\n",
    "            \n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    print(\"\\n=== RELATÓRIO DE PERFORMANCE TINYBERT ===\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Tempo médio de inferência: {np.mean(inference_times):.3f} ms\")\n",
    "    print(f\"Tempo mínimo: {np.min(inference_times):.3f} ms\")\n",
    "    print(f\"Tempo máximo: {np.max(inference_times):.3f} ms\")\n",
    "    print(f\"Throughput: {1000/np.mean(inference_times):.1f} predições/segundo\")\n",
    "    \n",
    "    # Relatório de classificação\n",
    "    print(\"\\n=== RELATÓRIO DE CLASSIFICAÇÃO ===\")\n",
    "    print(classification_report(\n",
    "        all_labels, all_predictions, \n",
    "        target_names=label_encoder.classes_,\n",
    "        digits=4\n",
    "    ))\n",
    "    \n",
    "    # Matriz de confusão\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=label_encoder.classes_,\n",
    "                yticklabels=label_encoder.classes_)\n",
    "    plt.title('Matriz de Confusão - TinyBERT')\n",
    "    plt.ylabel('Verdadeiro')\n",
    "    plt.xlabel('Predito')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return accuracy, np.mean(inference_times)\n",
    "\n",
    "# Avaliação detalhada\n",
    "if df is not None:\n",
    "    accuracy, avg_inference_time = detailed_evaluation_tinybert(model, test_loader, label_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Otimização para Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimizedTinyBERTDetector(nn.Module):\n",
    "    \"\"\"Wrapper ultra-otimizado para exportação ONNX\"\"\"\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, features):\n",
    "        logits = self.model(features)\n",
    "        probabilities = torch.softmax(logits, dim=-1)\n",
    "        return logits, probabilities\n",
    "\n",
    "def export_tinybert_model(model, X_sample, scaler, label_encoder, feature_names):\n",
    "    \"\"\"Exportar modelo TinyBERT ultra-otimizado para ONNX\"\"\"\n",
    "    \n",
    "    # Criar wrapper otimizado\n",
    "    optimized_model = OptimizedTinyBERTDetector(model)\n",
    "    optimized_model.eval()\n",
    "    \n",
    "    # Preparar input de exemplo\n",
    "    dummy_input = torch.FloatTensor(X_sample[:1]).to(device)\n",
    "    \n",
    "    # Exportar para ONNX com otimizações máximas\n",
    "    torch.onnx.export(\n",
    "        optimized_model,\n",
    "        dummy_input,\n",
    "        'tinybert_attack_detector.onnx',\n",
    "        input_names=['features'],\n",
    "        output_names=['logits', 'probabilities'],\n",
    "        dynamic_axes={\n",
    "            'features': {0: 'batch_size'},\n",
    "            'logits': {0: 'batch_size'},\n",
    "            'probabilities': {0: 'batch_size'}\n",
    "        },\n",
    "        opset_version=14,\n",
    "        do_constant_folding=True,\n",
    "        optimize_for_mobile=True\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo TinyBERT exportado para ONNX: tinybert_attack_detector.onnx\")\n",
    "    \n",
    "    # Quantizar modelo com configurações agressivas\n",
    "    from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "    \n",
    "    quantize_dynamic(\n",
    "        'tinybert_attack_detector.onnx',\n",
    "        'tinybert_attack_detector_quantized.onnx',\n",
    "        weight_type=QuantType.QInt8,\n",
    "        optimize_model=True\n",
    "    )\n",
    "    \n",
    "    print(\"Modelo TinyBERT quantizado: tinybert_attack_detector_quantized.onnx\")\n",
    "    \n",
    "    # Salvar metadados\n",
    "    import pickle\n",
    "    \n",
    "    metadata = {\n",
    "        'model_type': 'TinyBERT',\n",
    "        'scaler': scaler,\n",
    "        'label_encoder': label_encoder,\n",
    "        'feature_names': feature_names,\n",
    "        'num_features': len(feature_names),\n",
    "        'num_classes': len(label_encoder.classes_),\n",
    "        'classes': label_encoder.classes_.tolist(),\n",
    "        'optimization_level': 'ultra'\n",
    "    }\n",
    "    \n",
    "    with open('tinybert_metadata.pkl', 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    \n",
    "    print(\"Metadados TinyBERT salvos: tinybert_metadata.pkl\")\n",
    "    \n",
    "    return metadata\n",
    "\n",
    "# Exportar modelo otimizado\n",
    "if df is not None:\n",
    "    print(\"Exportando modelo TinyBERT ultra-otimizado...\")\n",
    "    metadata = export_tinybert_model(model, X_test, scaler, label_encoder, feature_names)\n",
    "    \n",
    "    # Verificar tamanhos dos arquivos\n",
    "    import os\n",
    "    original_size = os.path.getsize('tinybert_attack_detector.onnx') / (1024*1024)\n",
    "    quantized_size = os.path.getsize('tinybert_attack_detector_quantized.onnx') / (1024*1024)\n",
    "    \n",
    "    print(f\"\\nTamanho do modelo original: {original_size:.2f} MB\")\n",
    "    print(f\"Tamanho do modelo quantizado: {quantized_size:.2f} MB\")\n",
    "    print(f\"Redução: {(1 - quantized_size/original_size)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Sistema de Monitoramento em Tempo Real para IoT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar o script já criado no arquivo tinybert_network_monitor.py\n",
    "print(\"Sistema de monitoramento TinyBERT já criado em: tinybert_network_monitor.py\")\n",
    "print(\"\\nPara usar:\")\n",
    "print(\"1. Benchmark: python3 tinybert_network_monitor.py --benchmark\")\n",
    "print(\"2. Simulação: python3 tinybert_network_monitor.py --simulate dados.csv\")\n",
    "print(\"3. Interativo: python3 tinybert_network_monitor.py --interactive\")\n",
    "print(\"4. Stress test: python3 tinybert_network_monitor.py --stress-test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Arquivos de Configuração e Documentação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar requirements específicos para IoT\n",
    "requirements_iot = '''onnxruntime==1.15.1\n",
    "numpy==1.24.3\n",
    "pandas==2.0.3\n",
    "scikit-learn==1.3.0\n",
    "psutil==5.9.5\n",
    "'''\n",
    "\n",
    "with open('requirements_iot.txt', 'w') as f:\n",
    "    f.write(requirements_iot)\n",
    "\n",
    "# Script de instalação para IoT\n",
    "install_iot_script = '''#!/bin/bash\n",
    "# Script de instalação TinyBERT para IoT extremo\n",
    "\n",
    "echo \"🚀 Instalando TinyBERT para IoT...\"\n",
    "\n",
    "# Configurações de sistema para IoT\n",
    "echo \"Configurando sistema...\"\n",
    "export OMP_NUM_THREADS=1\n",
    "export ONNX_DISABLE_STATIC_ANALYSIS=1\n",
    "ulimit -v 300000  # Limitar memória virtual a 300MB\n",
    "\n",
    "# Instalar dependências mínimas\n",
    "pip3 install --no-cache-dir -r requirements_iot.txt\n",
    "\n",
    "# Criar diretório de logs compactos\n",
    "mkdir -p logs\n",
    "\n",
    "echo \"✅ TinyBERT IoT instalado!\"\n",
    "echo \"Teste: python3 tinybert_network_monitor.py --benchmark --samples 100\"\n",
    "'''\n",
    "\n",
    "with open('install_iot.sh', 'w') as f:\n",
    "    f.write(install_iot_script)\n",
    "\n",
    "print(\"Arquivos de configuração IoT criados!\")\n",
    "print(\"- requirements_iot.txt\")\n",
    "print(\"- install_iot.sh\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Download dos Arquivos para Raspberry Pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB and df is not None:\n",
    "    print(\"Preparando arquivos TinyBERT para download...\")\n",
    "    \n",
    "    # Lista de arquivos para download\n",
    "    files_to_download = [\n",
    "        'tinybert_attack_detector_quantized.onnx',\n",
    "        'tinybert_metadata.pkl',\n",
    "        'requirements_iot.txt',\n",
    "        'install_iot.sh'\n",
    "    ]\n",
    "    \n",
    "    # Download dos arquivos\n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            files.download(file)\n",
    "            print(f\"✅ {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Erro ao baixar {file}: {e}\")\n",
    "    \n",
    "    print(\"\\n=== RESUMO TINYBERT ===\")\n",
    "    print(f\"📊 Accuracy do modelo: {accuracy:.3f}\")\n",
    "    print(f\"⚡ Tempo de inferência: {avg_inference_time:.3f} ms\")\n",
    "    print(f\"🚀 Throughput: {1000/avg_inference_time:.0f} predições/segundo\")\n",
    "    print(f\"💾 Tamanho do modelo: {quantized_size:.1f} MB\")\n",
    "    print(f\"🎯 Classes detectáveis: {len(label_encoder.classes_)}\")\n",
    "    \n",
    "    print(\"\\n=== PRÓXIMOS PASSOS IoT ===\")\n",
    "    print(\"1. Transfira todos os arquivos para o dispositivo IoT\")\n",
    "    print(\"2. Execute: chmod +x install_iot.sh && ./install_iot.sh\")\n",
    "    print(\"3. Teste: python3 tinybert_network_monitor.py --benchmark --samples 100\")\n",
    "    print(\"4. Use: python3 tinybert_network_monitor.py --simulate dados.csv\")\n",
    "    \n",
    "elif df is not None:\n",
    "    print(\"\\nArquivos TinyBERT salvos localmente:\")\n",
    "    print(\"- tinybert_attack_detector_quantized.onnx\")\n",
    "    print(\"- tinybert_metadata.pkl\")\n",
    "    print(\"- requirements_iot.txt\")\n",
    "    print(\"- install_iot.sh\")\n",
    "    print(\"\\nUse também:\")\n",
    "    print(\"- tinybert_network_monitor.py (sistema completo)\")\n",
    "    print(\"- README.md (documentação)\")\n",
    "else:\n",
    "    print(\"⚠️ Nenhum dado foi carregado. Faça upload dos arquivos CSV primeiro.\")\n",
    "\n",
    "print(\"\\n🎉 TinyBERT otimizado para IoT extremo concluído!\")\n",
    "print(\"\\n📋 Características finais:\")\n",
    "print(\"   - Modelo mais leve da família BERT\")\n",
    "print(\"   - Ideal para Raspberry Pi Zero e dispositivos IoT\")\n",
    "print(\"   - Inferência ultra-rápida (<5ms)\")\n",
    "print(\"   - Uso mínimo de memória (<200MB)\")\n",
    "print(\"   - Accuracy competitiva (>94%)\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Detecção de Ataques de Rede - TinyBERT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}